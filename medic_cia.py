import numpy as np
import streamlit as st
import requests
import os
import logging
import time
import random
from dotenv import load_dotenv

# Enhanced logging for better debugging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Set page config first (must be at the top)
st.set_page_config(
    page_title="AI Symptom Analysis Assistant",
    page_icon="🩺",
    layout="wide"
)

# Hugging Face API settings
API_URL_RECOGNITION = "https://api-inference.huggingface.co/models/jonatasgrosman/wav2vec2-large-xlsr-53-english"

# Using reliable, free models that don't require special permissions
# Primary medical diagnosis model options in order of preference
DIAGNOSTIC_MODELS = [
    "https://api-inference.huggingface.co/models/gpt2",  # Reliable fallback for all text generation
    "https://api-inference.huggingface.co/models/openai-community/gpt2-large"  # Larger version if available
]
DIAGNOSTIC_MODEL_API = DIAGNOSTIC_MODELS[0]  # Start with the primary model

api_key = os.getenv('HUGGINGFACE_API_KEY')
headers = {"Authorization": f"Bearer {api_key}"}

# Check for API key
if not api_key:
    st.error("⚠ Please set your HUGGINGFACE_API_KEY in the Streamlit Cloud secrets.")
    st.stop()

st.title("🩺 AI Symptom Analysis Assistant")
st.markdown("""
### How this works
This application uses AI to analyze your symptoms and provide possible insights. 
**Note:** This is a **demonstration only** - the AI's responses are generated by a general-purpose language model, not a specialized medical model.
For real medical advice, please consult a qualified healthcare professional.
""")

# History management
if "history" not in st.session_state:
    st.session_state.history = []

# Add a retry function for API calls
def call_huggingface_api_with_retry(api_url, headers, data=None, json_data=None, max_retries=3):
    """Call Hugging Face API with retry logic for handling 503 errors"""
    retry_count = 0
    backoff_time = 1  # Start with 1 second backoff
    
    while retry_count < max_retries:
        try:
            if json_data:
                response = requests.post(api_url, headers=headers, json=json_data)
            else:
                response = requests.post(api_url, headers=headers, data=data)
            
            # If success or not a 503 error, return immediately
            if response.status_code != 503:
                return response
            
            # Log the retry attempt
            retry_count += 1
            logger.warning(f"Received 503 error from API. Retry {retry_count}/{max_retries}")
            
            if retry_count < max_retries:
                # Add jitter to backoff time (avoid thundering herd problem)
                sleep_time = backoff_time + random.uniform(0, 0.5)
                logger.info(f"Waiting {sleep_time:.2f} seconds before retry...")
                time.sleep(sleep_time)
                # Exponential backoff
                backoff_time *= 2
            else:
                logger.error("Max retries reached. API might be unavailable.")
                return response
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error: {str(e)}")
            retry_count += 1
            if retry_count < max_retries:
                time.sleep(backoff_time)
                backoff_time *= 2
            else:
                # Create a custom response to indicate network error
                class ErrorResponse:
                    def __init__(self):
                        self.status_code = 0
                        self.text = str(e)
                return ErrorResponse()
    
    # This should never be reached, but just in case
    class FinalErrorResponse:
        def __init__(self):
            self.status_code = 500
            self.text = "Maximum retries exceeded"
    return FinalErrorResponse()

# Function to try different models until one works
def try_diagnosis_models(input_text):
    """Try multiple diagnosis models in sequence until one works"""
    for model_url in DIAGNOSTIC_MODELS:
        logger.info(f"Trying model: {model_url}")
        
        # Format the query as a medical consultation prompt
        formatted_input = f"Patient: I am feeling {input_text}\nDoctor: Based on your symptoms, "
        
        response = call_huggingface_api_with_retry(
            model_url, 
            headers, 
            json_data={"inputs": formatted_input, "parameters": {"max_length": 150, "temperature": 0.7}}
        )
        
        # If the response is successful, return it
        if response.status_code == 200:
            return response, model_url.split('/')[-1]
        
        # Log the failed attempt
        logger.warning(f"Model {model_url} returned status code {response.status_code}")
    
    # If all models fail, return the last response and None for model_name
    return response, None

# Process diagnosis helper function
def process_diagnosis(text, is_text_input=False):
    """Process diagnosis for given text and handle API response appropriately"""
    # Use text_input parameter name if this was called from text input field
    input_text = text
    
    # Try multiple models until one works
    response, model_name = try_diagnosis_models(input_text)
    
    try:
        # First check if response has valid status code
        if response.status_code == 200:
            # Check if response content is valid JSON
            try:
                result_json = response.json()
                
                # Extract the generated text based on the response format
                if isinstance(result_json, list) and len(result_json) > 0:
                    # Handle list responses
                    if 'generated_text' in result_json[0]:
                        result = result_json[0]['generated_text']
                    else:
                        result = str(result_json[0])
                elif isinstance(result_json, dict):
                    # Handle dictionary responses
                    if 'generated_text' in result_json:
                        result = result_json['generated_text']
                    else:
                        result = str(result_json)
                else:
                    # Handle string or other response types
                    result = str(result_json)
                
                # Clean up the result for GPT-2 outputs
                # Extract just the doctor's response
                if "Doctor: Based on your symptoms, " in result:
                    parts = result.split("Doctor: Based on your symptoms, ")
                    if len(parts) > 1:
                        result = parts[1].strip()
                        
                        # Remove any follow-up patient questions
                        if "Patient:" in result:
                            result = result.split("Patient:")[0].strip()
                
                # Ensure result doesn't exceed reasonable length
                if len(result) > 500:
                    result = result[:500] + "..."
                
                st.success(f"🧠 Diagnosis (via {model_name}): {result}")
                
                # Update history
                st.session_state.history.append({"message": input_text, "is_user": True})
                st.session_state.history.append({"message": result, "is_user": False})
                return True, result
            except ValueError as json_err:
                st.error(f"Failed to parse API response: {str(json_err)}")
                logger.error(f"JSON parsing error: {str(json_err)}, Response: {response.text[:100]}")
                st.info("Response details:")
                st.code(response.text[:500])
                return False, None
        else:
            st.error(f"All models failed with errors")
            st.error("The generative AI service is currently unavailable. Please try again later.")
            st.info("Response details:")
            st.code(response.text[:500] if hasattr(response, 'text') else "No response text available")
            return False, None
    except Exception as e:
        st.error(f"Diagnosis failed: {str(e)}")
        logger.error(f"General error in diagnosis processing: {str(e)}")
        return False, None

# Create a tab layout for text input and audio upload
tab1, tab2 = st.tabs(["⌨ Text Input", "🎤 Voice Upload"])

with tab1:
    st.subheader("Describe your symptoms")
    st.markdown("""
    Please provide details about your symptoms such as:
    - What symptoms are you experiencing?
    - When did they start?
    - Are they getting better or worse?
    """)
    
    # Text input for symptoms
    text_input = st.text_area("Type your symptoms here:", height=150)
    
    if text_input and st.button("Get Diagnosis", key="text_diagnosis"):
        with st.spinner("Analyzing your symptoms..."):
            process_diagnosis(text_input, is_text_input=True)

with tab2:
    st.subheader("Upload an audio recording")
    st.markdown("""
    ### Instructions:
    1. Record yourself describing your symptoms using any recording app
    2. Save the recording as a WAV file
    3. Upload the file below
    4. Click 'Analyze Audio' to get a diagnosis
    """)
    
    # Audio file upload
    uploaded_file = st.file_uploader("Upload audio file (WAV format)", type=["wav"])
    
    if uploaded_file is not None:
        st.audio(uploaded_file)
        if st.button("Analyze Audio"):
            with st.spinner("Processing audio... (this may take a moment)"):
                try:
                    response = call_huggingface_api_with_retry(
                        API_URL_RECOGNITION, 
                        headers, 
                        data=uploaded_file.getvalue()
                    )
                    
                    if response.status_code == 200:
                        text = response.json().get("text", "Speech recognition failed")
                        st.success(f"🗣 Transcription: {text}")
                        
                        # Get diagnosis based on transcribed text
                        process_diagnosis(text)
                    else:
                        st.error(f"Speech recognition API error: {response.status_code}")
                        st.error("The Hugging Face service is currently unavailable. Please try again later.")
                        st.info("Response details:")
                        st.code(response.text[:500] if hasattr(response, 'text') else "No response text available")
                except Exception as e:
                    st.error(f"Error processing audio: {str(e)}")
                    logger.error(f"Audio processing error: {str(e)}")

# Display consultation history
if st.session_state.history:
    st.subheader("Consultation History")
    history_container = st.container()
    
    with history_container:
        for chat in st.session_state.history:
            if chat["is_user"]:
                st.markdown(f"**👤 You:** {chat['message']}")
            else:
                st.markdown(f"**🤖 AI Assistant:** {chat['message']}")
    
    # Add a button to clear history
    if st.button("Clear History"):
        st.session_state.history = []
        st.experimental_rerun()
else:
    st.info("Your consultation history will appear here after you get a diagnosis.")

# Add a prominent disclaimer at the bottom
st.markdown("---")
st.error("""
**IMPORTANT MEDICAL DISCLAIMER**

This application uses a general-purpose AI language model (GPT-2) that has NOT been specifically trained on medical data. 
The information provided should be treated as a demonstration of AI capabilities only.

⚠️ The responses may not be medically accurate, relevant, or appropriate for your situation.

**Always consult with a qualified healthcare provider for medical diagnosis, advice, and treatment.**
""")

# Add information about the model
st.markdown("""
**About this demo:**
- This app uses GPT-2, a general text generation model that has no specialized medical training
- Audio transcription is provided by Hugging Face's speech recognition model
- This is for demonstration purposes only
""")
    
